{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9abb595b-551c-4430-b460-e33d9646ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\esra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CÃ¼mle 1 - Lemmatized: ['frida', 'kahlo', 'magdalena', 'carmen', 'frida', 'kahlo', 'calderÃ³n', 'spanish', 'pronunciation', 'ËˆfÉ¾iÃ°a', 'Ëˆkalo', 'july', 'july', 'mexican', 'painter', 'known', 'many', 'portrait', 'work', 'inspired', 'nature', 'artifact', 'mexico']\n",
      "CÃ¼mle 1 - Stemmed: ['frida', 'kahlo', 'magdalena', 'carmen', 'frida', 'kahlo', 'calderÃ³n', 'spanish', 'pronunci', 'ËˆfÉ¾iÃ°a', 'Ëˆkalo', 'juli', 'juli', 'mexican', 'painter', 'known', 'mani', 'portrait', 'work', 'inspir', 'natur', 'artifact', 'mexico']\n",
      "\n",
      "\n",
      "CÃ¼mle 2 - Lemmatized: ['inspired', 'country', 'popular', 'culture', 'employed', 'naÃ¯ve', 'folk', 'art', 'style', 'explore', 'question', 'identity', 'postcolonialism', 'gender', 'class', 'race', 'mexican', 'society']\n",
      "CÃ¼mle 2 - Stemmed: ['inspir', 'countri', 'popular', 'cultur', 'employ', 'naÃ¯v', 'folk', 'art', 'style', 'explor', 'question', 'ident', 'postcoloni', 'gender', 'class', 'race', 'mexican', 'societi']\n",
      "\n",
      "\n",
      "CÃ¼mle 3 - Lemmatized: ['painting', 'often', 'strong', 'autobiographical', 'element', 'mixed', 'realism', 'fantasy']\n",
      "CÃ¼mle 3 - Stemmed: ['paint', 'often', 'strong', 'autobiograph', 'element', 'mix', 'realism', 'fantasi']\n",
      "\n",
      "\n",
      "CÃ¼mle 4 - Lemmatized: ['addition', 'belonging', 'mexicayotl', 'movement', 'sought', 'define', 'mexican', 'identity', 'kahlo', 'described', 'surrealist', 'magical', 'realist']\n",
      "CÃ¼mle 4 - Stemmed: ['addit', 'belong', 'mexicayotl', 'movement', 'sought', 'defin', 'mexican', 'ident', 'kahlo', 'describ', 'surrealist', 'magic', 'realist']\n",
      "\n",
      "\n",
      "CÃ¼mle 5 - Lemmatized: ['also', 'known', 'painting', 'experience', 'chronic', 'pain']\n",
      "CÃ¼mle 5 - Stemmed: ['also', 'known', 'paint', 'experi', 'chronic', 'pain']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Dosyadan veri okuma\n",
    "with open(\"sanatcilar_biyografi.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()  # DosyanÄ±n tÃ¼m iÃ§eriÄŸini al\n",
    "\n",
    "# CÃ¼mlelere ayÄ±rma\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Lemmatizer ve Stemmer'Ä± baÅŸlat\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stopwords listesini almak\n",
    "nltk.download('stopwords')  # EÄŸer stopwords yÃ¼klenmediyse\n",
    "stop_words = set(stopwords.words('english'))  # EÄŸer metin TÃ¼rkÃ§e ise 'turkish' kullan\n",
    "\n",
    "# Kelimeleri tokenleÅŸtirip, lemmatize etme ve stemleme\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)  # CÃ¼mleyi kelimelere ayÄ±r\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatize etme\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]  # Stemleme\n",
    "    \n",
    "    return lemmatized_tokens, stemmed_tokens\n",
    "\n",
    "# Her cÃ¼mleyi tokenleÅŸtir, lemmatize et ve stemle\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    lemmatized_tokens, stemmed_tokens = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)\n",
    "\n",
    "# Ä°lk 5 cÃ¼mleyi yazdÄ±ralÄ±m\n",
    "for i in range(min(5, len(tokenized_corpus_lemmatized))):\n",
    "    print(f\"CÃ¼mle {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n",
    "    print(f\"CÃ¼mle {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4402cf87-f86d-45f5-9f91-bfcd7f2a877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\esra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_model_cbow_window2_dim100 model saved!\n",
      "lemmatized_model_skipgram_window2_dim100 model saved!\n",
      "lemmatized_model_cbow_window4_dim100 model saved!\n",
      "lemmatized_model_skipgram_window4_dim100 model saved!\n",
      "lemmatized_model_cbow_window2_dim300 model saved!\n",
      "lemmatized_model_skipgram_window2_dim300 model saved!\n",
      "lemmatized_model_cbow_window4_dim300 model saved!\n",
      "lemmatized_model_skipgram_window4_dim300 model saved!\n",
      "stemmed_model_cbow_window2_dim100 model saved!\n",
      "stemmed_model_skipgram_window2_dim100 model saved!\n",
      "stemmed_model_cbow_window4_dim100 model saved!\n",
      "stemmed_model_skipgram_window4_dim100 model saved!\n",
      "stemmed_model_cbow_window2_dim300 model saved!\n",
      "stemmed_model_skipgram_window2_dim300 model saved!\n",
      "stemmed_model_cbow_window4_dim300 model saved!\n",
      "stemmed_model_skipgram_window4_dim300 model saved!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# ðŸ“Œ Wikipedia yerine sanatÃ§Ä± biyografi dosyasÄ±nÄ± okuyoruz\n",
    "with open(\"sanatcilar_biyografi.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()  # DosyanÄ±n tÃ¼m iÃ§eriÄŸini al\n",
    "\n",
    "# CÃ¼mlelere ayÄ±rma\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Lemmatizer ve Stemmer'Ä± baÅŸlat\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stopwords listesini almak\n",
    "nltk.download('stopwords')  # EÄŸer yÃ¼klenmemiÅŸse\n",
    "stop_words = set(stopwords.words('english'))  # TÃ¼rkÃ§e iÃ§in 'turkish' kullanabilirsin\n",
    "\n",
    "# ðŸ”¹ Kelimeleri iÅŸleme fonksiyonu\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)  # CÃ¼mleyi kelimelere ayÄ±r\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatize etme\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]  # Stemleme\n",
    "    \n",
    "    return lemmatized_tokens, stemmed_tokens\n",
    "\n",
    "# ðŸ”¹ Metni iÅŸleyerek tokenizasyon yapÄ±yoruz\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    lemmatized_tokens, stemmed_tokens = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)\n",
    "\n",
    "# ðŸ“Œ Word2Vec modeli eÄŸitmek iÃ§in parametreler\n",
    "parameters = [\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]\n",
    "\n",
    "# ðŸ”¹ Word2Vec modelini eÄŸitme ve kaydetme fonksiyonu\n",
    "def train_and_save_model(corpus, params, model_name):\n",
    "    model = Word2Vec(corpus, vector_size=params['vector_size'], window=params['window'], min_count=1, sg=1 if params['model_type'] == 'skipgram' else 0)\n",
    "    model.save(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}.model\")\n",
    "    print(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']} model saved!\")\n",
    "\n",
    "# ðŸ“Œ Lemmatize edilmiÅŸ corpus ile modelleri eÄŸitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_lemmatized, param, \"lemmatized_model\")\n",
    "\n",
    "# ðŸ“Œ StemlenmiÅŸ corpus ile modelleri eÄŸitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_stemmed, param, \"stemmed_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15e811-d419-412f-b3fd-7ebc0e9d1450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
